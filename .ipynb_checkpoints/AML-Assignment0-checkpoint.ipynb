{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import textract \n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import operator\n",
    "from os import listdir\n",
    "from os import path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "def plot_hist():\n",
    "    #Plot the result \n",
    "    figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.bar(range(len(top_50)), list(top_50.values()), align='center')\n",
    "    plt.xticks(range(len(top_50)), list(top_50.keys()), fontsize=18, rotation='vertical')\n",
    "    \n",
    "def remove_punctuation_and_stop_words():\n",
    "    # Get rid of all the punctuations here i keep the apostrophe. \n",
    "    remove_punctuations = re.sub(\"[^\\w'\\s]\",'',extracted_text)\n",
    "    # extract tokens \n",
    "    # For word tokenization contractions are considered two words because meaning-wise they are.\n",
    "    # using tweet_tokenizer to avoid contractions ref: https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    extracted_tokens = tweet_tokenizer.tokenize(remove_punctuations)\n",
    "\n",
    "    #set stop-words for english\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #list of filtered tokens for the current book\n",
    "    filtered_tokens = [a for a in extracted_tokens if not a.lower() in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def select_top_50():\n",
    "    #create word to frequency for all the tokens\n",
    "    word_to_freq = Counter(filtered_tokens)\n",
    "\n",
    "    #sort by frequency of all the tokens\n",
    "    sorted_dict = dict(sorted(word_to_freq.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    #select top 50.\n",
    "    top_50 = dict(itertools.islice(sorted_dict.items(),0,49))\n",
    "    return top_50\n",
    "\n",
    "\n",
    "def read_files(d)\n",
    "    extracted_text = \"\"\n",
    "    for x in listdir(d):\n",
    "        print(\"Reading file ... \",x)\n",
    "        extracted_text += textract.process(path.join(d,x)).decode('utf-8')\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "extracted_text = textract.process(\"data/A_Dolls_House_by_Henrik_Ibsen.rtf\").decode('utf-8')\n",
    "\n",
    "print(type(extracted_text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aiyangar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aiyangar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment0-aml",
   "language": "python",
   "name": "assignment0-aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
