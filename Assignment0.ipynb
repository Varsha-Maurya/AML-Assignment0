{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import textract\n",
    "from matplotlib.pyplot import figure\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def plot_hist(top_50):\n",
    "    #Plot the result \n",
    "    figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.bar(range(len(top_50)), list(top_50.values()), align='center')\n",
    "    plt.xticks(range(len(top_50)), list(top_50.keys()), fontsize=18, rotation='vertical')\n",
    "    \n",
    "def remove_punctuation_and_stop_words(extracted_text):\n",
    "    # Get rid of all the punctuations here i keep the apostrophe. \n",
    "    print('Removing punctuations and stop words')\n",
    "    remove_punctuations = re.sub(\"[^\\w'\\s]\",'',extracted_text)\n",
    "    # extract tokens \n",
    "    # For word tokenization contractions are considered two words because meaning-wise they are.\n",
    "    # using tweet_tokenizer to avoid contractions ref: https://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    extracted_tokens = tweet_tokenizer.tokenize(remove_punctuations)\n",
    "\n",
    "    #set stop-words for english\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #list of filtered tokens for the current book\n",
    "    filtered_tokens = [a for a in extracted_tokens if not a.lower() in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def select_top_50(filtered_tokens):\n",
    "    print('Selecting the top 50 words')\n",
    "    #create word to frequency for all the tokens\n",
    "    word_to_freq = Counter(filtered_tokens)\n",
    "\n",
    "    #sort by frequency of all the tokens\n",
    "    sorted_dict = dict(sorted(word_to_freq.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "    #select top 50.\n",
    "    top_50 = dict(itertools.islice(sorted_dict.items(),0,49))\n",
    "    return top_50\n",
    "\n",
    "\n",
    "def read_files(d):\n",
    "    extracted_text = \"\"\n",
    "    for x in listdir('data'):\n",
    "        print(\"Reading file .... \", x)\n",
    "        extracted_text += textract.process(path.join(d, x)).decode('utf-8')\n",
    "        print(\"The file size loaded currently ...\", sys.getsizeof(extracted_text))\n",
    "    return extracted_text\n",
    "    \n",
    "#### MAIN #####\n",
    "\n",
    "text = read_files('data')\n",
    "\n",
    "tokens = remove_punctuation_and_stop_words(text)\n",
    "\n",
    "top_50 = select_top_50(tokens)\n",
    "\n",
    "plot_hist(top_50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment0-aml",
   "language": "python",
   "name": "assignment0-aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
